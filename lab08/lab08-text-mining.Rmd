---
title: "Lab 08 - Text Mining/NLP"
author: "Elaine Dai"
date: "2024-2-28"
output: html_document
always_allow_html: true
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = T, include  = T)
```


View the rendered HTML version of the lab [here](https://htmlpreview.github.io/?https://github.com/Vantion24601/JSC370-labs/blob/master/lab08/lab08-text-mining.html)


# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://github.com/JSC370/JSC370-2024/tree/main/data/medical_transcriptions.

This markdown document should be rendered using `github_document` document.



### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`. Install `wordcloud`, `tm`, and `topicmodels` if you don't alreadyh have them.

```{r message=FALSE}
# install.packages("tidytext")
# install.packages("wordcloud")
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("webshot2")
library(dplyr)
library(ggplot2)
library(kableExtra)
library(webshot2)
```




### Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r message=FALSE}
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r }
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(reorder(medical_specialty, n), n)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
mt_samples |> count(medical_specialty, sort = TRUE)
```
There are 30 categories, not evenly distributed.

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  count(word, sort = TRUE)

tokens |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(word, n), y = n)) +
  labs(y = "Count", x = "Word") +
  geom_bar(stat = "identity")

wordcloud(tokens$word, tokens$n, max.words = 100)
```

The words with top occurrence are mostly common stop words, which makes sense. We need to remove the stop words in order to get insight from the data. 


---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords(use stopwords package)
- Bonus points if you remove numbers as well (use regex)

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

```{r}
head(stopwords("english"))
length(stopwords("english"))
```


```{r}
stopwords2 <- c(stopwords("english"), "also", "one", "two")
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>
  filter(!grepl("[[:digit:]]+", word)) |>
  count(word, sort = TRUE)

tokens |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(word, n), y = n)) +
  labs(y = "Count", x = "Word") +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

wordcloud(tokens$word, tokens$n, max.words = 100)

```

We see that the most frequent words after removing stop words are more medical-related and provides more insight to the context.

---



# Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r}
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")
```


```{r}
# bigrams
tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!grepl(sw_start, ngram, ignore.case = TRUE))|>
  filter(!grepl(sw_end, ngram, ignore.case = TRUE))|>
  filter(!grepl("[[:digit:]]+", ngram))|>
  count(ngram, sort = TRUE)
```

```{r}
# trigrams
tokens_trigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 3) |>
  filter(!grepl(sw_start, ngram, ignore.case = TRUE))|>
  filter(!grepl(sw_end, ngram, ignore.case = TRUE))|>
  filter(!grepl("^[[:digit:]]+|[[:digit:]]$", ngram))|>
  count(ngram, sort = TRUE)
```


```{r}
# plot the bigrams
tokens_bigram |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(ngram, n), y = n)) +
  labs(y = "Count", x = "Bigram") +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# plot the trigrams
tokens_trigram |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(ngram, n), y = n)) +
  labs(y = "Count", x = "Trigram") +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Trigram phrases provide richer context compared to bigrams. 



---

# Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r }
library(stringr)
```


```{r }
# Words before "patient"
before <- tokens_bigram |>
  filter(str_detect(ngram, regex("\\spatient$"))) |>
  mutate(word = str_remove(ngram, "patient"),
         word = str_remove_all(word, " ")) |>
  group_by(word) |>
  summarise(n = sum(n)) |>
  arrange(desc(n))
before
```

```{r}
before |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(word, n), y = n)) +
  labs(y = "Count", x = 'Word before "patient"') +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



```{r}
# Words after "patient"
after <- tokens_bigram |>
  filter(str_detect(ngram, regex("^patient\\s"))) |>
  mutate(word = str_remove(ngram, "patient"),
         word = str_remove_all(word, " ")) |>
  group_by(word) |>
  summarise(n = sum(n)) |>
  arrange(desc(n))
after
```

```{r}
after |>
  slice_max(order_by = n, n = 20) |>
  ggplot(aes(reorder(word, n), y = n)) +
  labs(y = "Count", x = 'Word after "patient"') +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

---


# Question 6: Words by Specialties

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?


```{r }
spe_top1 <- mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>
  filter(!grepl("[[:digit:]]+", word)) |>
  group_by(medical_specialty) |>
  count(word, sort = TRUE) |>
  slice_max(order_by = n, n = 1)

kable(spe_top1, "html", position = "right") |>
  kable_styling("striped", full_width = FALSE) |>
  scroll_box(width = "100%", height = "500px")
```



```{r }
spe_top5 <- mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>
  filter(!grepl("[[:digit:]]+", word)) |>
  group_by(medical_specialty) |>
  count(word, sort = TRUE) |>
  slice_max(order_by = n, n = 5)

kable(spe_top5, "html", position = "right") |>
  kable_styling("striped", full_width = FALSE) |>
  scroll_box(width = "100%", height = "500px")

```

# Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r }
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords2) |>
  filter(!grepl("[[:digit:]]+", word)) |>
  DocumentTermMatrix()

transcripts_dtm <- as.matrix(transcripts_dtm)
```


```{r }
transcripts_lda5 <- LDA(transcripts_dtm, k = 5, control = list(seed = 1234))
transcripts_lda3 <- LDA(transcripts_dtm, k = 3, control = list(seed = 1234))
```


```{r}
lda5_top_terms <- 
  tidy(transcripts_lda5, matrix = "beta") |>
  group_by(topic) |>
  slice_max(order_by = beta, n = 10) |>
  ungroup() |>
  arrange(topic, -beta) |>
  mutate(term = reorder_within(term, beta, topic))
lda5_top_terms
```

```{r}
lda3_top_terms <- 
  tidy(transcripts_lda3, matrix = "beta") |>
  group_by(topic) |>
  slice_max(order_by = beta, n = 10) |>
  ungroup() |>
  arrange(topic, -beta) |>
  mutate(term = reorder_within(term, beta, topic))
lda3_top_terms
```

```{r}
lda5_top_terms |>
  ggplot(aes(x = beta, y = term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free", nrow = 3) +
  scale_y_reordered()
```

```{r}
lda3_top_terms |>
  ggplot(aes(x = beta, y = term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free", nrow = 2) +
  scale_y_reordered()
```



# Deliverables

1. Questions 1-7 answered, raw .Rmd file and pdf or html output uploaded to Quercus
